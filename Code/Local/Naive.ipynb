{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime,  timedelta\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    " \n",
    "from matplotlib import pyplot\n",
    "\n",
    "from solarpy import irradiance_on_plane\n",
    "from solarpy import solar_panel\n",
    "from numpy import array\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D, RepeatVector, Lambda, TimeDistributed, Embedding, TimeDistributed, BatchNormalization, Reshape, concatenate, Permute\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.backend import square, mean\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "%run Database.ipynb\n",
    "%run AuxFunctions.ipynb\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(20,7), 'figure.dpi':300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cololect building data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv, df_consumption = collectFromDatabase(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfinal = getData(local = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Build diferent dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, a2, a3, a4, a5, v1, v2, v3, v4, t5, d6 = getDatasets(dfinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 - Define the train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Naive-3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = a4.copy()\n",
    "dataset = '2'\n",
    "\n",
    "final_features = features()\n",
    "dt = dt[final_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_val = v4.copy()\n",
    "\n",
    "final_features = features()\n",
    "dt_val = dt_val[final_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dt.values\n",
    "val = dt_val.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 - Build supervised learning sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pre = 60\n",
    "n_post = 15\n",
    "\n",
    "dX, dY = [], []\n",
    "for i in range(len(train)-n_pre-n_post):\n",
    "    dX.append(train[i:i+n_pre])\n",
    "trainX = np.array(dX)\n",
    "\n",
    "\n",
    "for i in range(len(train)-n_pre-n_post):\n",
    "\n",
    "    ar = np.array(([row[12] for row in train[i+n_pre:i+n_pre+n_post]][4], [row[12] for row in train[i+n_pre:i+n_pre+n_post]][9], [row[12] for row in train[i+n_pre:i+n_pre+n_post]][14]))\n",
    "    B = ar.reshape(-1, len(ar))\n",
    "    dY.append(B)\n",
    "    \n",
    "trainY = np.array(dY)\n",
    "\n",
    "\n",
    "vX, vY = [], []\n",
    "for i in range(len(val)-n_pre-n_post):\n",
    "    vX.append(val[i:i+n_pre])\n",
    "valX = np.array(vX)\n",
    "\n",
    "for i in range(len(val)-n_pre-n_post):\n",
    "\n",
    "    ar = np.array(([row[12] for row in val[i+n_pre:i+n_pre+n_post]][4], [row[12] for row in val[i+n_pre:i+n_pre+n_post]][9], [row[12] for row in val[i+n_pre:i+n_pre+n_post]][14]))\n",
    "    B = ar.reshape(-1, len(ar))\n",
    "    vY.append(B)\n",
    "\n",
    "valY = np.array(vY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_original = trainX.copy()\n",
    "trainY_original = trainY.copy()\n",
    "valX_original = valX.copy()\n",
    "valY_original = valY.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = trainY.reshape(trainY.shape[0], trainY.shape[2])\n",
    "valY = valY.reshape(valY.shape[0], valY.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainX.shape, trainY.shape, valX.shape, valY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 - Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalersX = {}\n",
    "\n",
    "for i in range(trainX.shape[2]):\n",
    "    scalersX[i] = MinMaxScaler(feature_range=(0, 1))\n",
    "    trainX[:, :, i] = scalersX[i].fit_transform(trainX[:, :, i]) \n",
    "\n",
    "for i in range(valX.shape[2]):\n",
    "    valX[:, :, i] = scalersX[i].transform(valX[:, :, i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerY = MinMaxScaler(feature_range=(0, 1))\n",
    "trainY = scalerY.fit_transform(trainY) \n",
    "valY = scalerY.transform(valY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 - Perform predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = Naive_Predict(valX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = predict.reshape(len(predict), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_original = predict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_array = np.empty((n_pre - 1))\n",
    "nan_array.fill(np.nan)\n",
    "nan_array2 = np.empty(n_post)\n",
    "nan_array2.fill(np.nan)\n",
    "ind = np.arange(n_pre + n_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = scalerY.inverse_transform(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 - Plot the predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = np.full([valX_original.shape[0],1,15], np.nan)\n",
    "\n",
    "aux2 = np.full([predict.shape[0],1,15], np.nan)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 3.5))\n",
    "for i in range(600, valX.shape[0], valX.shape[0]):\n",
    "\n",
    "    aux[i, 0, 4] = valY_original[i, 0, 0]\n",
    "    aux[i, 0, 9] = valY_original[i, 0, 1]\n",
    "    aux[i, 0, 14] = valY_original[i, 0, 2]\n",
    "    \n",
    "    aux2[i, 0, 4] = predict[i, 0]\n",
    "    aux2[i, 0, 9] = predict[i, 1]\n",
    "    aux2[i, 0, 14] = predict[i, 2]\n",
    "    \n",
    "    forecasts_original = np.concatenate((nan_array, valX_original[i, -1:, 7], aux2[i, 0, :]))\n",
    "    ground_truth = np.concatenate((nan_array, valX_original[i, -1:, 7], aux[i, 0, :]))\n",
    "    network_input = np.concatenate((valX_original[i, :, 7], nan_array2))\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    SMALLER_SIZE = 18\n",
    "    SMALL_SIZE = 19\n",
    "    MEDIUM_SIZE = 23\n",
    "    BIGGER_SIZE = 25\n",
    "\n",
    "    #plt.('test title', fontsize=BIGGER_SIZE)\n",
    "    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=BIGGER_SIZE)    # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=SMALL_SIZE)     # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=SMALLER_SIZE)  # legend fontsize\n",
    "    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "    \n",
    "    \n",
    "    ax.plot(ind, ground_truth, 'r-o', markersize=10, markeredgecolor='black', linewidth=2, label='Av. Power')\n",
    "    \n",
    "    ax.plot(ind, forecasts_original, 'go--', markersize=10, linewidth=2, marker='h', markerfacecolor='lightgreen', \\\n",
    "             markeredgewidth=2, label='Forecast')\n",
    "            \n",
    "    ax.plot(ind[40:], network_input[40:], '-o', markersize=10, markeredgecolor='black', linewidth=2, label='Av. Power')\n",
    "    \n",
    " \n",
    "    \n",
    "    plt.ticklabel_format(axis='y', style='sci', scilimits=(5,5))\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Available Power [W]')\n",
    "    plt.title('Model 2 - Available Power - Forecast')\n",
    "    plt.legend(loc='lower left')\n",
    "    #plt.savefig('Images/' + model_name, bbox_inches = 'tight')\n",
    "    plt.savefig('Images/' + model_name + '_op2', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 - Get arrays with the predictions of only 5, 10 and 5 minutes ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_normalized = []\n",
    "original_normalized = []\n",
    "\n",
    "predicted = []\n",
    "original = []\n",
    "\n",
    "aux = np.full([valY_original.shape[0],1,15], np.nan)\n",
    "aux5 = np.full([valY.shape[0],1,15], np.nan)\n",
    "aux1 = np.full([predict.shape[0],1,15], np.nan)\n",
    "aux2 = np.full([predict.shape[0],1,15], np.nan)\n",
    "\n",
    "for i in range(0, valX.shape[0]):\n",
    "\n",
    "    aux[i, 0, 4] = valY_original[i, 0, 0]\n",
    "    aux[i, 0, 9] = valY_original[i, 0, 1]\n",
    "    aux[i, 0, 14] = valY_original[i, 0, 2]\n",
    "\n",
    "    aux1[i, 0, 4] = predict_original[i, 0]\n",
    "    aux1[i, 0, 9] = predict_original[i, 1]\n",
    "    aux1[i, 0, 14] = predict_original[i, 2]\n",
    "\n",
    "    aux2[i, 0, 4] = predict[i, 0]\n",
    "    aux2[i, 0, 9] = predict[i, 1]\n",
    "    aux2[i, 0, 14] = predict[i, 2]\n",
    "\n",
    "    aux5[i, 0, 4] = valY[i, 0]\n",
    "    aux5[i, 0, 9] = valY[i, 1]\n",
    "    aux5[i, 0, 14] = valY[i, 2]\n",
    "\n",
    "    forecasts_normalized = np.concatenate((nan_array, valX[i, -1:, 7], aux1[i, 0, :]))\n",
    "    forecasts = np.concatenate((nan_array, valX_original[i, -1:, 7], aux2[i, 0, :]))\n",
    "\n",
    "    ground_truth_normalized = np.concatenate((nan_array, valX[i, -1:, 7], aux5[i, 0, :]))\n",
    "    ground_truth = np.concatenate((nan_array, valX_original[i, -1:, 7], aux[i, 0, :]))\n",
    "\n",
    "    predicted_normalized.append((forecasts_normalized[n_pre+4], forecasts_normalized[n_pre+9], forecasts_normalized[n_pre+14]))\n",
    "    predicted.append((forecasts[n_pre+4], forecasts[n_pre+9], forecasts[n_pre+14]))\n",
    "\n",
    "    original_normalized.append((ground_truth_normalized[n_pre+4], ground_truth_normalized[n_pre+9], ground_truth_normalized[n_pre+14]))\n",
    "    original.append((ground_truth[n_pre+4], ground_truth[n_pre+9], ground_truth[n_pre+14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 - Compute errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse_5, test_rmse_10, test_rmse_15,\\\n",
    "test_mse_5, test_mse_10, test_mse_15,\\\n",
    "test_mae_5, test_mae_10, test_mae_15 = printTestErrors(original, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse_5_n, test_rmse_10_n, test_rmse_15_n,\\\n",
    "test_mse_5_n, test_mse_10_n, test_mse_15_n,\\\n",
    "test_mae_5_n, test_mae_10_n, test_mae_15_n = printTestErrorsNormalized(original_normalized, predicted_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.9 - Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Test_' + dataset + '.csv', 'a', newline='') as file:\n",
    "    fieldnames = ['Model', \n",
    "                  'Test_MSE_n', 'Test_RMSE_n', 'Test_MAE_n',                  \n",
    "                  'Test_5_RMSE_n', 'Test_10_RMSE_n', 'Test_15_RMSE_n', \n",
    "                  'Test_5_MSE_n', 'Test_10_MSE_n', 'Test_15_MSE_n', \n",
    "                  'Test_5_MAE_n', 'Test_10_MAE_n', 'Test_15_MAE_n',\n",
    "                  \n",
    "                  'Test_5_RMSE', 'Test_10_RMSE', 'Test_15_RMSE', \n",
    "                  'Test_5_MSE', 'Test_10_MSE', 'Test_15_MSE', \n",
    "                  'Test_5_MAE', 'Test_10_MAE', 'Test_15_MAE']\n",
    "    \n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "with open('Test_' + dataset + '.csv', 'a', newline='') as file:\n",
    "    fieldnames = ['Model',\n",
    "                  'Test_MSE_n', 'Test_RMSE_n', 'Test_MAE_n',                  \n",
    "                  'Test_5_RMSE_n', 'Test_10_RMSE_n', 'Test_15_RMSE_n', \n",
    "                  'Test_5_MSE_n', 'Test_10_MSE_n', 'Test_15_MSE_n', \n",
    "                  'Test_5_MAE_n', 'Test_10_MAE_n', 'Test_15_MAE_n',\n",
    "                \n",
    "                  'Test_5_RMSE', 'Test_10_RMSE', 'Test_15_RMSE', \n",
    "                  'Test_5_MSE', 'Test_10_MSE', 'Test_15_MSE', \n",
    "                  'Test_5_MAE', 'Test_10_MAE', 'Test_15_MAE']\n",
    "    \n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "       \n",
    "    writer.writerow({'Model': model_name, \n",
    "                     \n",
    "                      \n",
    "                     \n",
    "                     'Test_5_RMSE_n': test_rmse_5_n,\n",
    "                     'Test_10_RMSE_n': test_rmse_10_n,\n",
    "                     'Test_15_RMSE_n': test_rmse_15_n, \n",
    "                     'Test_5_MSE_n': test_mse_5_n,\n",
    "                     'Test_10_MSE_n': test_mse_10_n,\n",
    "                     'Test_15_MSE_n': test_mse_15_n, \n",
    "                     'Test_5_MAE_n': test_mae_5_n,\n",
    "                     'Test_10_MAE_n': test_mae_10_n,\n",
    "                     'Test_15_MAE_n': test_mae_15_n,\n",
    "                                     \n",
    "                     'Test_5_RMSE': test_rmse_5,\n",
    "                     'Test_10_RMSE': test_rmse_10,\n",
    "                     'Test_15_RMSE': test_rmse_15, \n",
    "                     'Test_5_MSE': test_mse_5,\n",
    "                     'Test_10_MSE': test_mse_10,\n",
    "                     'Test_15_MSE': test_mse_15, \n",
    "                     'Test_5_MAE': test_mae_5,\n",
    "                     'Test_10_MAE': test_mae_10,\n",
    "                     'Test_15_MAE': test_mae_15                 \n",
    "                                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
