{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime,  timedelta\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    " \n",
    "from matplotlib import pyplot\n",
    "\n",
    "from solarpy import irradiance_on_plane\n",
    "from solarpy import solar_panel\n",
    "from numpy import array\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D, RepeatVector, Lambda, TimeDistributed, Embedding, TimeDistributed, BatchNormalization, Reshape, concatenate, Permute\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.backend import square, mean\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "%run Database.ipynb\n",
    "%run AuxFunctions.ipynb\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(20,5), 'figure.dpi':300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect from EDP database - True / Collect from local files - False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv, df_consumption = collectFromDatabase(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfinal = getData(local = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = np.genfromtxt('dados__Rad.dat', delimiter=',', skip_header=3, missing_values='Missing',names=True,dtype=None)\n",
    "dataset2 = np.genfromtxt('dados__Met.dat', delimiter=',', skip_header=3, missing_values='Missing',names=True,dtype=None)\n",
    "\n",
    "df1 = pd.DataFrame(data=dataset1)\n",
    "df2 = pd.DataFrame(data=dataset2)\n",
    "\n",
    "df1.columns = [\"Timestamp\",\"RECORD\",\"Avg_GHI\",\"Avg_DHI\",\"Avg_POA\",\"Avg_DNI\",\"Avg_cosDNI\",\"Avg_closGHI\",\"Std_GHI\",\"Std_DHI\",\"Std_POA\",\"Std_DNI\",\"Std_cosDNI\",\"Std_closGHI\",\"Min_GHI\",\"Min_DHI\",\"Min_POA\",\"Min_DNI\",\"Min_cosDNI\",\"Min_closGHI\",\"Max_GHI\",\"Max_DHI\",\"Max_POA\",\"Max_DNI\",\"Max_cosDNI\",\"Max_closGHI\",\"phi\",\"theta\",\"SensorT\"]\n",
    "df2.columns = [\"Timestamp\",\"RECORD\",\"T_amb_min\",\"T_amb_max\",\"T_amb_avg\",\"T_dp_avg\",\"RH_min\",\"RH_max\",\"RH_avg\",\"AH_min\",\"AH_max\",\"AH_avg\",\"p_amb_min\",\"p_amb_max\",\"p_amb_avg\",\"rho_act\",\"v_min\",\"v_max\",\"v_avg\",\"v_vectavg\",\"v_dir_min\",\"v_dir_max\",\"v_dir_vectavg\"]\n",
    "\n",
    "df1['Timestamp'] = pd.to_datetime(df1['Timestamp'].str.decode(\"utf-8\"), format='\"%Y-%m-%d %H:%M:%S\"')\n",
    "df2['Timestamp'] = pd.to_datetime(df2['Timestamp'].str.decode(\"utf-8\"), format='\"%Y-%m-%d %H:%M:%S\"')\n",
    "df3 = pd.merge(df1,df2, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection and data type selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PV = df_pv.copy()\n",
    "consumption = df_consumption.copy()\n",
    "\n",
    "consumption = consumption.set_index('Timestamp')\n",
    "df3.rename(columns={\"Timestamp_x\": \"Timestamp\", \"RECORD_x\": \"Record\"}, inplace=True)\n",
    "\n",
    "PV['Timestamp'] = PV['Timestamp'].dt.round('min')\n",
    "PV = PV.groupby('Timestamp').mean().reset_index()\n",
    "PV = PV.set_index('Timestamp')\n",
    "\n",
    "idx = pd.period_range(PV.index[0], PV.index[-1], freq='min')\n",
    "idy = pd.period_range(consumption.index[0], consumption.index[-1], freq='min')\n",
    "\n",
    "PV = PV.reset_index()\n",
    "PV = PV.set_index('Timestamp').resample(\"min\").first().reset_index().reindex(columns=PV.columns)\n",
    "cols = PV.columns.difference(['I1', 'I2', 'I3', 'V1', 'V2', 'V3', 'ActPwr'])\n",
    "PV[cols] = PV[cols].ffill()\n",
    "PV['ActPwr'] = PV['ActPwr']*1000\n",
    "\n",
    "consumption = consumption.reset_index()\n",
    "consumption = consumption.set_index('Timestamp').resample(\"min\").first().reset_index().reindex(columns=consumption.columns)\n",
    "cols = consumption.columns.difference(['Ir','Is','It','Vrs','Vst','Vtr','P','S'])\n",
    "consumption[cols] = consumption[cols].ffill()\n",
    "\n",
    "consumption = consumption.set_index('Timestamp')\n",
    "df3 = df3.set_index('Timestamp')\n",
    "\n",
    "PV_original = PV.copy()\n",
    "PV = PV_original[['Timestamp', 'ActPwr']].copy()\n",
    "\n",
    "consumption_original = consumption.copy()\n",
    "consumption = consumption_original[['P']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sun irradiation theoretical computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = solar_panel(500, 0.15, id_name='EDP')  # surface, efficiency and name\n",
    "panel.set_orientation(array([0, 0, -1]))  # upwards\n",
    "panel.set_position(38.707089, -9.148882, 0)  # LISBON latitude, longitude, altitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PV = PV.reset_index()\n",
    "PV['Theoretical Value'] = PV['Timestamp'].apply(lambda x: fnc(x.year, x.month, x.day, x.hour, x.minute))# year, month, day, hour, minute\n",
    "PV = PV.set_index('Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PV['Theoretical Value'] = PV['Theoretical Value'].shift(60, axis = 0) #passar tudo para UTC\n",
    "PV = PV.replace(np.nan, '-1')\n",
    "#PV.to_csv(\"PV.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge meteo and PV datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREV = df4 = pd.merge(df3, PV, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREV = PREV.drop(columns=['Timestamp_y', 'RECORD_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREV = PREV[['Avg_DHI', 'Avg_GHI', 'Avg_DNI', 'Avg_POA', \\\n",
    "             'T_amb_avg','RH_avg', 'AH_avg', 'p_amb_avg', 'rho_act', 'v_avg', 'v_dir_vectavg',    \\\n",
    "             'Theoretical Value', 'ActPwr']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREV = PREV.replace('-1', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data interpolation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding 0's when there is no sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOITE = PREV.copy()\n",
    "NOITE['ActPwr_noite'] = NOITE.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataaux = NOITE.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolations for gaps of less than 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INT = NOITE.copy()\n",
    "INT['Interpolation'] = INT['ActPwr_noite'].interpolate(method='polynomial', limit=30, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataaux['Interpolation'] = INT['Interpolation'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INT['Interpolation'] = INT['Interpolation'].replace(np.nan, '-1000')\n",
    "INT['Interpolation_Final'] = INT.apply(s, axis=1)\n",
    "INT['Interpolation'] = INT['Interpolation'].replace('-1000',np.nan)\n",
    "\n",
    "print(INT.ActPwr.count()/INT['Theoretical Value'].count() * 100)\n",
    "print(INT.ActPwr_noite.count()/INT['Theoretical Value'].count() * 100)\n",
    "print(INT.Interpolation.count()/INT['Theoretical Value'].count() * 100)\n",
    "print(INT.Interpolation_Avg_GHI.count()/INT['Theoretical Value'].count() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dfinal -> Merge PV and Consumption datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INT = INT.drop(['ActPwr', 'ActPwr_noite', 'Interpolation'], axis=1)\n",
    "INT = INT.rename(columns={\"Interpolation_Final\": \"ActPwr\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption = consumption.interpolate(method='polynomial', limit=30, order=2)\n",
    "\n",
    "dfinal = pd.concat([INT, consumption], axis=1, sort = False, join = 'inner')\n",
    "dfinal = dfinal.rename(columns={\"Interpolation_Final\": \"ActPwr\"})\n",
    "dfinal = dfinal.reset_index()\n",
    "dfinal = dfinal.set_index('Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataaux['ActPwr_final'] = dfinal['ActPwr'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data completion visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(20,10), 'figure.dpi':300})\n",
    "font = {'weight' : 'normal','size'   : 28}\n",
    "plt.rc('font', **font)\n",
    "pyplot.figure()\n",
    "\n",
    "dataaux['ActPwr'].plot(label='Phase 5', linewidth=4, color='red')\n",
    "dataaux['Theoretical Value'].plot(label='Phase 4', linewidth=1, color='blue',)\n",
    "\n",
    "plt.title('Production Active Power')\n",
    "plt.xlim('02/24/2020 06:30', '02/24/2020 19:00')\n",
    "plt.ylim(-5,65000 )\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('[W]')\n",
    "plt.legend()\n",
    "axes = plt.gca()\n",
    "plt.grid()\n",
    "plt.savefig(\"int0\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, a2, a3, a4, a5, v1, v2, v3, v4, t5, d6 = getDatasets(dfinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = a5.copy()\n",
    "dataset = 'a5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dt.values # getting all values as a matrix of dataframe \n",
    "sc = StandardScaler() # creating a StandardScaler object\n",
    "X_std = sc.fit_transform(X) # standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_pca = pca.fit(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 15\n",
    "pca = PCA(num_components)  \n",
    "X_pca = pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 0.99)\n",
    "X_pca = pca.fit_transform(X_std) # this will fit and reduce dimensions\n",
    "print(pca.n_components_) # one can print and see how many components are selected. In this case it is 4 same as above we saw in step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.components_, columns = dt.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pcs= pca.n_components_ # get number of component\n",
    "# get the index of the most important feature on EACH component\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = dt.columns\n",
    "# get the most important feature names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_names.pop(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_names.pop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = a5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_selected = dt[final_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(30, 20))\n",
    "ax = sns.heatmap(dt_selected.corr(), linewidths=3.5, annot=True, annot_kws={\"size\": 25}, fmt=\".2f\", cbar=True, cbar_kws = dict({\"shrink\": 1},use_gridspec=False,location=\"top\"))\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 90, fontsize = 30)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation = 0, fontsize = 30)\n",
    "\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=30)\n",
    "\n",
    "\n",
    "ax.figure.savefig(\"corr1.png\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
